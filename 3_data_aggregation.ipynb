{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preparation\n",
    "\n",
    "*Date: August 1, 2023*  \n",
    "*Author: Alicia Larsen*     \n",
    "*Institution: The Research Institute of Sweden (RISE)*   \n",
    "*Contact: alicia.hh.larsen@gmail.com*  \n",
    "\n",
    "This is the 4th notebook of 7, in the series \"RISE Wildfire Prediction Using Machine Learning\"\n",
    "##### Keywords: LST, LSR, Fire detection, MODIS, Python\n",
    "\n",
    "## Reference\n",
    "This notebook is based on the procedures in the notebook found on this [link](https://github.com/ornldaac/modis_restservice_qc_filter_Python/blob/master/modis_restservice_qc_filter_Python.ipynb). This notebook can also be found in /initial-eda/data-procurement/reference-notebook/download-modis-data-example-notebook.ipynb, on github.com:larsenalicia/RISE-wildfire-prediction.git\n",
    "\n",
    "## Overview\n",
    "The norebook will handle the following:\n",
    "* Standardisation of spatial and temporal resolution.\n",
    "* Value and unit derivations\n",
    "* Area differentiation\n",
    "* Normalization of features\n",
    "\n",
    "## Prerequisites: \n",
    "\n",
    "* Python 2 or 3   \n",
    "* Libraries: requests, json, datetime, pandas, numpy, matplotlib\n",
    "* Having run 1_data_procurement.ipynb, and have the resulting csv files in the directory /data/..\n",
    "\n",
    "---\n",
    "## Set-up\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from globals.global_vars import bands, coordinate_description, start_year, end_year, products, data_points_in_time_interval_8, product_names, time_intervals, alternative_mapping, types, date_indices, original_dimensions, space_resolution, above_below_left_right\n",
    "from procerdures.b_aggregation import half_time_interval, double_time_interval, resolution_decrement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load required data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes: dict = {}\n",
    "\n",
    "# Iterate through the different filtering restrictions\n",
    "for restriction in ['hard', 'loose']:\n",
    "\n",
    "    # Iterate through the different products\n",
    "    for product in product_names:\n",
    "        try:\n",
    "            # Read a CSV in the right directory\n",
    "            df_data = pd.read_csv(f'data/filtered/{restriction}/{product}_{start_year}-{end_year}_{coordinate_description}.csv').rename(columns={'Unnamed: 0': 'date'})\n",
    "\n",
    "            # Add the dataframe to a dictionary, for access\n",
    "            dataframes[f'{product}_{restriction}'] = df_data\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "dataframes.keys()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize time-intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the dataframes start with the same date\n",
    "first_index = []\n",
    "for df in dataframes.values():\n",
    "    first_index.append(df.loc[0, 'date'])\n",
    "\n",
    "len(set(first_index)) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_time_interval_least_frequent: dict = {}\n",
    "dataframes_time_interval_most_frequent: dict = {}\n",
    "\n",
    "with open ('globals/most_frequent.ob', 'rb') as fp:\n",
    "    most_frequent_index = pickle.load(fp)\n",
    "\n",
    "# Iterate through the keys for the dataframes\n",
    "for key in dataframes:\n",
    "\n",
    "    # Remove 'hard', and 'loose' from the key\n",
    "    key_core = key.split('_')[0]\n",
    "    \n",
    "    # Some core-keys have multiple data-bands, and then the we need a unique key that identified the band\n",
    "    if key_core in products:\n",
    "        pass\n",
    "    else:\n",
    "        key_core = alternative_mapping[key_core]\n",
    "\n",
    "    # Find out to what intervals we want to convert the input-dataframe\n",
    "    least_freq_interval = int(max(time_intervals.values()))\n",
    "    most_freq_interval = int(min(time_intervals.values()))\n",
    "\n",
    "    # Determine the type (max/mean), e.g. max for ´fire´, and mean for all numerical data.\n",
    "    type = types[key_core]\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Standardize the *least* frequent time interval \n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # Define the dataframe\n",
    "    df = dataframes[key]\n",
    "\n",
    "    # If the dataframe already has this time interval, then append it as it is\n",
    "    if int(time_intervals[products[key_core]]) == least_freq_interval:\n",
    "        dataframes_time_interval_least_frequent[key] = df\n",
    "    # Otherwise:\n",
    "    else:\n",
    "        # Assumption: the date-intervals from modis will always be 2^n.\n",
    "        # Half the dataframe data-index until it has the same index as the max-interval dataframe.\n",
    "        while len(df.index) > date_indices[least_freq_interval]:\n",
    "            dataframes_time_interval_least_frequent[key] = half_time_interval(df, type)\n",
    "            df = dataframes_time_interval_least_frequent[key]\n",
    "\n",
    "    # ------------------------------------------------\n",
    "    # Standardize the *most* frequent time interval \n",
    "    # ------------------------------------------------\n",
    "    \n",
    "    # Define the dataframe\n",
    "    df = dataframes[key]\n",
    "\n",
    "    # If the dataframe already has this time interval, then append it as it is\n",
    "    if int(time_intervals[products[key_core]]) == most_freq_interval:\n",
    "        dataframes_time_interval_most_frequent[key] = df\n",
    "    # Otherwise:\n",
    "    else:\n",
    "        # Assumption: the date-intervals from modis will always be 2^n.\n",
    "        # Double the dataframe data-index until it has the same index as the max-interval dataframe.\n",
    "\n",
    "        while len(df.index) < len(most_frequent_index):\n",
    "            dataframes_time_interval_most_frequent[key] = double_time_interval(df, most_frequent_index)\n",
    "            df = dataframes_time_interval_most_frequent[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dataframes in the most frequent dataframe are the same, and if all frames where appended\n",
    "index_len_long = []\n",
    "for df in dataframes_time_interval_most_frequent.values():\n",
    "    index_len_long.append(len(df.index))\n",
    "len(set(index_len_long)) == 1 and len(dataframes_time_interval_most_frequent.keys()) == len(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all dataframes in the less frequent dataframe are the same, and if all frames where appended\n",
    "index_len_short = []\n",
    "for df in dataframes_time_interval_least_frequent.values():\n",
    "    index_len_short.append(len(df.index))\n",
    "len(set(index_len_short)) == 1 and len(dataframes_time_interval_most_frequent.keys()) == len(dataframes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary containing all the dataframes\n",
    "time_standardized_dataframes: dict = {'least_freq': dataframes_time_interval_least_frequent, 'most_freq': dataframes_time_interval_most_frequent}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the number of pixels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before stacking the dataframes to a two-level multiindex, we will reduce the number of pixels in the <code>df_nir</code> and <code>df_swir</code> dataframes. Since the NMDI has a higher resolution than the other two, the pixels will be aggregated to have equal resolution for all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_dataframes: dict = {}\n",
    "total = (len(dataframes_time_interval_least_frequent.keys())+len(dataframes_time_interval_most_frequent.keys()))\n",
    "\n",
    "# Iterate through the list of dictionares\n",
    "for time_key in time_standardized_dataframes: \n",
    "    dict_ = time_standardized_dataframes[time_key]\n",
    "\n",
    "    # Iterate through every key, which identifies the dataframe\n",
    "    for key in dict_:\n",
    "        \n",
    "        # Remove 'hard', and 'loose' from the key\n",
    "        key_core = key.split('_')[0]\n",
    "\n",
    "        # Define the dataframe and dimensions\n",
    "        df = dataframes[key]\n",
    "        desired_dimension = min(original_dimensions.values())\n",
    "        current_dimension = original_dimensions[key_core]\n",
    "\n",
    "        # Only decrease the dimension if it is not already the desired (lowest)\n",
    "        if desired_dimension == current_dimension:\n",
    "            standardized_dataframes[f'{time_key}_{key}'] = df\n",
    "        else:\n",
    "            standardized_dataframes[f'{time_key}_{key}'] = resolution_decrement(df, current_dimension, desired_dimension)\n",
    "        \n",
    "        print(f'✅ {key} {len(standardized_dataframes.keys())}/{total}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_standardized_dataframes: dict = {}\n",
    "\n",
    "# Make every dataframe stacked \n",
    "for key in standardized_dataframes:\n",
    "    formatted_standardized_dataframes[key] = standardized_dataframes[key].set_index('date').stack().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that everything looks right\n",
    "df_test = list(formatted_standardized_dataframes.values())[2]\n",
    "df_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Land Surface Temperature (LST): Kelvin to Celsius\n",
    "Although the Kelvin-scaled dateframe will be used for the data mining methods, we will preapre a celsiu-based dataframe for exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "celsius_dataframes: dict = {}\n",
    "\n",
    "# Iterate through the keys in the most recent dictionary of dataframes\n",
    "for key in formatted_standardized_dataframes:\n",
    "    \n",
    "    # Retrieve the ´predictor´\n",
    "    core_key = key.split('_')[2]\n",
    "\n",
    "    # If the ´predictor´ is land surface temperature (LST), then:\n",
    "    if core_key == 'lst':\n",
    "\n",
    "        # Define the dataframe, calculate celsius from Kelvin, and store the dataframe in ´celsius_dataframes´\n",
    "        df_lst = formatted_standardized_dataframes[key]\n",
    "        df_lst_celsius = (df_lst - 272.15)\n",
    "        celsius_dataframes[key] = df_lst_celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the resulting dictionary and add the dataframes to ´formatted_standardized_dataframes´\n",
    "for key in celsius_dataframes:  \n",
    "    \n",
    "    # Define the dataframe and the relevant variables\n",
    "    df = celsius_dataframes[key]\n",
    "    key_lst = key.split('_')\n",
    "    key_lst[2] = 'celsius'\n",
    "    new_key = '_'.join(key_lst)\n",
    "\n",
    "    # Store the dataframe in ´formatted_standardized_dataframes´\n",
    "    formatted_standardized_dataframes[new_key] = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surface reflectance in NIR and SWIR to NDMI Index\n",
    "The disired index to use as a predictor in the analysis is the normalised difference moisture index (NDMI) index. \n",
    "\n",
    "The current data is the percentage of surface reflectance in the near infrared (NIR) and the short wave-length infrared (SWIR) bands. These bands can be used to calculate the NDMI:\n",
    "\n",
    "<code>NDMI = (NIR - SWIR) / (NIR + SWIR)</code>\n",
    "\n",
    "Let's make a function describing this formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function of NDMI formula\n",
    "def ndmi(nir, swir):\n",
    "    ndmi = (nir - swir) / (nir + swir)\n",
    "    return ndmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndmi_dataframes: dict = {}\n",
    "seen: list = []\n",
    "\n",
    "# Iterate through the dataframe-identifiers (dictionary keys) in ´formatted_standardized_dataframes´\n",
    "for key1 in formatted_standardized_dataframes.keys():\n",
    "    \n",
    "    # Disect the key, to access key-words\n",
    "    key_lst1 = key1.split('_')\n",
    "\n",
    "    # Look for dataframes of 'NIR',\n",
    "    if key_lst1[2] == 'nir':\n",
    "        \n",
    "        # And check that the datafrane has not already been handled,\n",
    "        # If not, now you have \"seen\" it, and continue\n",
    "        if key not in seen:\n",
    "            seen.append(key1)\n",
    "\n",
    "            # Iterate through the  dataframe-identifiers (dictionary keys), \n",
    "            # to find the SWIR pair to NIR\n",
    "            for key2 in formatted_standardized_dataframes.keys():\n",
    "                key_lst2 = key2.split('_')\n",
    "\n",
    "                # If the keys are the same, apart from the predictor, which is \"swir\" for the second key, then:\n",
    "                if (key_lst1[0] == key_lst2[0]) and (key_lst1[1] == key_lst2[1]) and (key_lst2[2] == 'swir') and (key_lst1[3] == key_lst2[3]):\n",
    "                    seen.append(key2)       \n",
    "\n",
    "                    # Retrieve the dataframes for both keys\n",
    "                    df_nir = formatted_standardized_dataframes[key1]\n",
    "                    df_swir = formatted_standardized_dataframes[key2]\n",
    "                    \n",
    "                    # Calculate the NDMI, and store it in 'ndmi_dataframes'\n",
    "                    df_ndmi = ndmi(df_nir[0], df_swir[0]).to_frame()\n",
    "                    key_lst1[2] = 'ndmi'\n",
    "                    ndmi_dataframes['_'.join(key_lst1)] = df_ndmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the swir and nir dataframes from ´formatted_standardized_dataframes´\n",
    "for key in seen:\n",
    "    del formatted_standardized_dataframes[key]\n",
    "\n",
    "# Iterate through the resulting dictionary and add the dataframes to ´formatted_standardized_dataframes´\n",
    "for key in ndmi_dataframes:  \n",
    "    formatted_standardized_dataframes[key] = ndmi_dataframes[key]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change of fire detection classification\n",
    "The filtered data include:\n",
    "\n",
    "- 5: non-fire land pixel\n",
    "- 7: fire (low confidence, land or water)\n",
    "- 8: fire (nominal confidence, land or water)\n",
    "- 9: fire (high confidence, land or water)\n",
    "\n",
    "Let's renumber these into a binary schema, into two levels of restrictions. One that classifies \"low\" condidence fire as fire and one that classifies it as none-fire:\n",
    "\n",
    "More restricted aggregation:\n",
    "- 0: non-fire land pixel, low confidence of fire.\n",
    "- 1: fire (average/high confidence, land or water)\n",
    "\n",
    "Less restricted aggregation:\n",
    "- 0: non-fire land pixel\n",
    "- 1: fire (low/average/high confidence, land or water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_dataframes: dict = {}\n",
    "seen_fire = []\n",
    "\n",
    "# Iterate through the dataframe-identifiers (dictionary keys) in ´formatted_standardized_dataframes´\n",
    "for key in formatted_standardized_dataframes.keys():\n",
    "    \n",
    "    # Disect the key, to access key-words\n",
    "    key_lst = key.split('_')\n",
    "\n",
    "    # Look for dataframes of 'NIR',\n",
    "    if key_lst[2] == 'fire':\n",
    "\n",
    "        df = formatted_standardized_dataframes[key]\n",
    "\n",
    "        if key_lst[3] == 'hard':\n",
    "            df.loc[(df[0] <= 6.0)] = 0\n",
    "            df.loc[(df[0] > 6.0)] = 1\n",
    "        \n",
    "        elif key_lst[3] == 'loose':\n",
    "            df.loc[(df[0] <= 5.0)] = 0\n",
    "            df.loc[(df[0] > 5.0)] = 1\n",
    "        else:\n",
    "            raise ValueError('The naming of the dataframes are wrong')\n",
    "        \n",
    "        seen_fire.append(key)\n",
    "        fire_dataframes[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the swir and nir dataframes from ´formatted_standardized_dataframes´\n",
    "for key in seen_fire:\n",
    "    del formatted_standardized_dataframes[key]\n",
    "\n",
    "# Iterate through the resulting dictionary and add the dataframes to ´formatted_standardized_dataframes´\n",
    "for key in fire_dataframes:  \n",
    "    formatted_standardized_dataframes[key] = fire_dataframes[key]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate all data to one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check length of all dataframes\n",
    "print(f'''\n",
    "LENGTH OF DATAFRAMES\n",
    "--------------------\n",
    "temp: {len(formatted_standardized_dataframes['least_freq_lst_hard'].index)}\n",
    "ndmi: {len(formatted_standardized_dataframes['least_freq_ndmi_hard'].index)}\n",
    "evi:  {len(formatted_standardized_dataframes['least_freq_evi_hard'].index)}\n",
    "fire: {len(formatted_standardized_dataframes['least_freq_fire_hard'].index)}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_aggregated: dict = {}\n",
    "seen: list = []\n",
    "\n",
    "# Iterate through the dataframe-identifiers (dictionary keys) in ´formatted_standardized_dataframes´\n",
    "for key1 in formatted_standardized_dataframes.keys():\n",
    "\n",
    "    # Disect the key, to access key-words\n",
    "    key_lst1 = key1.split('_')\n",
    "\n",
    "    # Define core-words\n",
    "    frequency1 = key_lst1[0]\n",
    "    predictor1 = key_lst1[2]\n",
    "    restriction1 = key_lst1[3]\n",
    "    core_key1 = '_'.join([frequency1, restriction1])\n",
    "\n",
    "    # Let's use LST as key1\n",
    "    if predictor1 == 'lst':\n",
    "        print('lst')\n",
    "        # If the dataframe has not already been handled:\n",
    "        if core_key1 not in seen:    \n",
    "            seen.append(core_key1)\n",
    "            \n",
    "            # Define the initial dataframe (necessay to set multiindex, to concatenate later)\n",
    "            df_data = formatted_standardized_dataframes[key1].rename(columns={0: f'{predictor1}'})\n",
    "\n",
    "            # Iterate through the keys again,\n",
    "            for key2 in formatted_standardized_dataframes.keys():\n",
    "\n",
    "                # Disect the key, to access key-words\n",
    "                key_lst2 = key2.split('_')\n",
    "                frequency2 = key_lst2[0]\n",
    "                predictor2 = key_lst2[2]\n",
    "                restriction2 = key_lst2[3]\n",
    "                core_key2 = '_'.join([frequency2, restriction2]) # general\n",
    "\n",
    "                # Check if the categories of restriction, and frequency are the same, then:\n",
    "                if core_key1 == core_key2:\n",
    "                    \n",
    "                    # If the dataframe is not already handled, then:\n",
    "                    if predictor1 != predictor2:\n",
    "\n",
    "                        # Concatenate the dataframes \n",
    "                        df_data = pd.concat([df_data, formatted_standardized_dataframes[key2].rename(columns={0: f'{predictor2}'})], axis=1)\n",
    "                \n",
    "                elif predictor2 == 'ndmi' and (frequency1 == frequency2):\n",
    "                    # Concatenate the dataframes \n",
    "                    df_data = pd.concat([df_data, formatted_standardized_dataframes[key2].rename(columns={0: f'{predictor2}'})], axis=1)\n",
    "                \n",
    "            # Lastly, add every aggregated dataframe (per category) in a dictionary for storage\n",
    "            dataframes_aggregated[core_key1] = df_data\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that dataframes_aggregated has the epected keys\n",
    "dataframes_aggregated.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at an example\n",
    "df_test = dataframes_aggregated['least_hard']\n",
    "df_test.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_aggregated_normalized: dict = {}\n",
    "\n",
    "# Iterate through the dataframe identifiers (dictionary keys)\n",
    "for key in dataframes_aggregated.keys():\n",
    "    \n",
    "    # Define the dataframes\n",
    "    df_data = dataframes_aggregated[key]\n",
    "    df_normalized = pd.DataFrame()\n",
    "\n",
    "    # Normalize the columns in ´df_data´ and store it in ´df_normalized´\n",
    "    df_normalized['ndmi']=(df_data['ndmi']-df_data['ndmi'].mean())/df_data['ndmi'].std()\n",
    "    df_normalized['temperature_k']=(df_data['lst']-df_data['lst'].mean())/df_data['lst'].std()\n",
    "    df_normalized['temperature_c']=(df_data['celsius']-df_data['celsius'].mean())/df_data['celsius'].std()\n",
    "    df_normalized['evi']=(df_data['evi']-df_data['evi'].mean())/df_data['evi'].std()\n",
    "    df_normalized['fire']=df_data['fire']\n",
    "    \n",
    "\n",
    "    # Remove all NaN values\n",
    "    df_normalized = df_normalized.dropna()\n",
    "\n",
    "    # Store the resulting dataframe in ´dataframes_aggregated_normalized´\n",
    "    dataframes_aggregated_normalized[key] = df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataframe-identifiers\n",
    "for key in dataframes_aggregated_normalized:\n",
    "    series = dataframes_aggregated_normalized[key]['fire']\n",
    "    print(key, series.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the necessary function for calculating the dimension of an area.\n",
    "def dimension(above_below_left_right):\n",
    "    \"\"\" Takes a string key and calculates it's dimension in pixels.\n",
    "    \"\"\"\n",
    "    space_resolution_ = max(space_resolution.values())\n",
    "    return int((2 * above_below_left_right) / space_resolution_) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions of the future dataframes, trying to make every smaller dataframe, half the size of the first.\n",
    "dimension_largest = dimension(above_below_left_right)\n",
    "dimension_middle = int(np.ceil(((dimension(above_below_left_right)**2) / 2)**(1/2)))\n",
    "dimension_smallest = int((((dimension_middle)**2) / 2)**(1/2))\n",
    "\n",
    "print(f\"\"\"\n",
    "DIMENSIONS\n",
    "------------------------\n",
    "Largest dimension:    {dimension_largest}\n",
    "Middle dimension:     {dimension_middle}\n",
    "Smallest dimension:   {dimension_smallest}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that defines the pixel range\n",
    "def pixel_range(dimension_self):\n",
    "    \"\"\" Takes a dimension and calculates the pixels to include from a higher dimension.\n",
    "    \"\"\"\n",
    "    # Define relevant variables\n",
    "    padding = int((dimension_largest-dimension_self) / 2)\n",
    "    start_pixel = dimension_largest * padding\n",
    "    pixels_lst = []\n",
    "    recent_pix = start_pixel\n",
    "\n",
    "    # Iterate through the the rows of the new dataframe, and calculate the pixels to include, and save them\n",
    "    for _ in range(0, dimension_self):\n",
    "        pixles = list(range(recent_pix + padding-1, recent_pix + padding-1 + dimension_self))\n",
    "        pixels_lst += pixles\n",
    "        \n",
    "        # re-define the most recent pixel number\n",
    "        recent_pix += padding-1 + dimension_self + padding+1\n",
    "\n",
    "    # Check if the number of pixels were added as expected.\n",
    "    print(f'Expected number of pixels: {len(pixels_lst) == dimension_self**2}')\n",
    "\n",
    "    return pixels_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the calculations yielded the expected pixel ranges.\n",
    "dimension_largest\n",
    "pixels_middle = pixel_range(dimension_middle)\n",
    "pixels_smallest = pixel_range(dimension_smallest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that halves the size of a dataframe.\n",
    "def smaller_dataframe(df, pixels):\n",
    "    \"\"\" Takes a list of pixel numbers from current dimension, and returns a dataframe with half the size.\n",
    "    \"\"\"\n",
    "    # Convert every integer in list to str, to type-match the dataframe\n",
    "    pixels_string = map(str, pixels)\n",
    "\n",
    "    # Choose the desired rows\n",
    "    df = df.reset_index().set_index('date').rename(columns={'level_1': 'pixel'})\n",
    "    df = df[df.pixel.isin(pixels_string)]\n",
    "    df = df.reset_index().set_index(['date', 'pixel'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes: dict = {}\n",
    "pixel_nums: dict = {'largest': dimension_largest**2, 'middle': pixels_middle, 'smallest': pixels_smallest}\n",
    "\n",
    "# Iterate through the labels for each pixel-number-list\n",
    "for pix_name in pixel_nums:\n",
    "    \n",
    "    # Iterate through the dataframe identifiers in ´dataframes_aggregated_normalized´\n",
    "    for key in dataframes_aggregated_normalized:\n",
    "        \n",
    "        # Define the dataframe\n",
    "        df = dataframes_aggregated_normalized[key]\n",
    "\n",
    "        # If the dataframe does not have to go through the ´smaller_dataframe´ function, don't\n",
    "        if pix_name == 'largest':\n",
    "            dataframes[f'{key}_{pix_name}'] = df.reset_index().rename(columns={'level_1': 'pixel'}).set_index(['date', 'pixel']).rename(columns={'level_1': 'pixel'})\n",
    "        \n",
    "        # Otherwise, decrease the pize of the dataframe with ´smaller_dataframe´\n",
    "        else:\n",
    "            pixels = pixel_nums[pix_name]\n",
    "            dataframes[f'{key}_{pix_name}'] = smaller_dataframe(df, pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the dataframe-identifiers\n",
    "for key in dataframes:\n",
    "    series = dataframes[key]['fire']\n",
    "    print(key, series.unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the raw, but aggregated data to CSVs\n",
    "for key in dataframes_aggregated.keys():\n",
    "    df = dataframes_aggregated[key]\n",
    "    df.to_csv(f'data/aggregation/raw/alldata_{key}_{start_year}-{end_year}_{coordinate_description}.csv')\n",
    "\n",
    "# Save the normalized, and pixel-differentiated dataframes to CSVs\n",
    "for key in dataframes.keys():\n",
    "    df = dataframes[key]\n",
    "    df.to_csv(f'data/aggregation/normalized/alldata_{key}_{start_year}-{end_year}_{coordinate_description}.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "Now you should have pre-processed all datasets.\n",
    "\n",
    "Have a nice day!\n",
    "\n",
    "/ Alicia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rise-wildfires",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
