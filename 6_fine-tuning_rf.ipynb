{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Machine Learning Methods\n",
    " \n",
    "*Date: July 31, 2023*  \n",
    "*Author: Alicia Larsen*     \n",
    "*Institution: The Research Institute of Sweden (RISE)*   \n",
    "*Contact: alicia.hh.larsen@gmail.com*   \n",
    "\n",
    "This is the 7th notebook of 7, in the series \"RISE Wildfire Prediction Using Machine Learning\"\n",
    "\n",
    "References: This notebook is based on the procedures in the notebook found on this [link](https://github.com/ornldaac/modis_restservice_qc_filter_Python/blob/master/modis_restservice_qc_filter_Python.ipynb). This notebook can also be found in /initial-eda/data-procurement/reference-notebook/download-modis-data-example-notebook.ipynb, on github.com:larsenalicia/RISE-wildfire-prediction.git\n",
    "\n",
    "##### Keywords: LST, LSR, Fire, MODIS, Python\n",
    "\n",
    "## Overview\n",
    "This notebook will mainly explore under- and oversampling techniques.\n",
    "\n",
    "## Prerequisites: \n",
    "\n",
    "* Python 2 or 3   \n",
    "* Libraries: requests, json, datetime, pandas, numpy, matplotlib\n",
    "---\n",
    "\n",
    "## Set-up\n",
    "### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import 'LogisticRegression' and create a LogisticRegression object\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import 'RandomForestRegressor'\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Import modules to visualise the random forest\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "# SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# UnderSampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Import for Cross validation\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Visualisations\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from globals.global_vars import url, header, coordinate_description, lat, lon, start_year, end_year, products, bands, random_state, product_names\n",
    "from procerdures.d_model import performace_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "test_size = 0.33\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes: dict = {}\n",
    "\n",
    "# Iterate through the different frequences\n",
    "for frequency in ['least', 'most']:\n",
    "\n",
    "    # Iterate through the different filtering restrictions\n",
    "    for restriction in ['hard', 'loose']:\n",
    "\n",
    "        for size in ['largest', 'middle', 'smallest']:\n",
    "\n",
    "            # Read a CSV in the right directory\n",
    "            df_data = pd.read_csv(f'data/aggregation/normalized/alldata_{frequency}_{restriction}_{size}_{start_year}-{end_year}_{coordinate_description}.csv')\n",
    "\n",
    "            # Add the dataframe to a dictionary, for access\n",
    "            dataframes[f'{frequency}_{restriction}_{size}'] = df_data.rename(columns={'Unnamed: 0': 'date'}).set_index(['date', 'pixel'])\n",
    "\n",
    "# Take a look at the keys\n",
    "dataframes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the most promesing dataframe\n",
    "df_data = dataframes['least_hard_largest']\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataframe, and the idenpendent and depednent variables\n",
    "X = df_data[['temperature_k', 'ndmi', 'evi']].values\n",
    "y = df_data['fire'].values\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model without compensating for value imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 100, \n",
    "                            random_state = random_state)\n",
    "\n",
    "# Calculate the f1 scores\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = np.rint(rf.predict(X_test))\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compensate for value imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the value imbalance\n",
    "train_num_pos = list(y_train).count(1.0)\n",
    "train_num_neg = list(y_train).count(0.0)\n",
    "\n",
    "train_num_pos, train_num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 0\n",
    "undersampling_parameter = 0\n",
    "under_over_sampling = {'param': [], 'f1_un': []}\n",
    "\n",
    "for i in range(1, int(np.floor(train_num_neg**(1/3))), 1):\n",
    "\n",
    "    i = i**3\n",
    "\n",
    "    # Undersampling\n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy={0.0: i, 1.0: train_num_pos})\n",
    "    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Initialize the model\n",
    "    rf = RandomForestClassifier(n_estimators = 100, \n",
    "                            random_state = random_state)\n",
    "    \n",
    "    # Calculate the f1 scores\n",
    "    rf.fit(X_train_res, y_train_res)\n",
    "    y_pred = np.rint(rf.predict(X_test))\n",
    "    f1_new = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # Append the values to look at later\n",
    "    under_over_sampling['param'].append(i) \n",
    "    under_over_sampling['f1_un'].append(f1_new) \n",
    "\n",
    "    # Find the param with the best f1 score\n",
    "    if f1_new > f1:\n",
    "        f1 = f1_new\n",
    "        undersampling_parameter = i\n",
    "\n",
    "print(undersampling_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define df_data_ov as a dataframe\n",
    "df_data_ov = pd.DataFrame.from_dict(under_over_sampling)\n",
    "df_data_ov.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make line plot of model performance depending on undersampling\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(data=df_data_ov, x='param', y='f1_un', ax=ax)\n",
    "ax.set_xlim(-300, 0.1*10**6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oversampling\n",
    "#### Maximal oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 0\n",
    "under_over_sampling = {'param': [], 'f1_ov': []}\n",
    "\n",
    "# Oversampling\n",
    "sm = SMOTE(random_state=seed, sampling_strategy={0.0: train_num_neg, 1.0: train_num_neg})\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "# Initialize the model\n",
    "rf = RandomForestClassifier(n_estimators = 100, \n",
    "                        random_state = random_state)\n",
    "\n",
    "# Calculate the f1 scores\n",
    "rf.fit(X_train_res, y_train_res)\n",
    "y_pred = np.rint(rf.predict(X_test))\n",
    "f1 = f1_score(y_test, y_pred, average='binary')\n",
    "\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterative oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 0\n",
    "undersampling_parameter = 0\n",
    "under_over_sampling = {'param': [], 'f1_ov': []}\n",
    "\n",
    "for i in range(0, int(np.floor(train_num_neg**(1/4))), 1):\n",
    "\n",
    "    if i < train_num_pos:\n",
    "        i = train_num_pos\n",
    "    else:\n",
    "        i = i**4\n",
    "\n",
    "    # Oversampling\n",
    "    sm = SMOTE(random_state=seed, sampling_strategy={0.0: train_num_neg, 1.0: i})\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Initialize the model\n",
    "    rf = RandomForestClassifier(n_estimators = 100, \n",
    "                            random_state = random_state)\n",
    "    \n",
    "    # Calculate the f1 scores\n",
    "    rf.fit(X_train_res, y_train_res)\n",
    "    y_pred = np.rint(rf.predict(X_test))\n",
    "    f1_new = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # Append the values to look at later\n",
    "    under_over_sampling['param'].append(i) \n",
    "    under_over_sampling['f1_ov'].append(f1_new) \n",
    "\n",
    "    # Find the param with the best f1 score\n",
    "    if f1_new > f1:\n",
    "        f1 = f1_new\n",
    "        undersampling_parameter = i\n",
    "\n",
    "print(undersampling_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define df_data_un as a dataframe\n",
    "df_data_un = pd.DataFrame.from_dict(under_over_sampling)\n",
    "df_data_un.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make line plot of model performance depending on oversampling\n",
    "sns.lineplot(data=df_data_un, x='param', y='f1_ov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of Under- and Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = 0\n",
    "undersampling_parameter = 0\n",
    "under_over_sampling = {'param': [], 'f1_unov': []}\n",
    "\n",
    "for i in range(1, int(np.floor(train_num_neg**(1/2))), 1):\n",
    "\n",
    "    i = i**2\n",
    "\n",
    "    if i < train_num_pos:\n",
    "        i = train_num_pos\n",
    "\n",
    "    # Undersampling\n",
    "    rus = RandomUnderSampler(random_state=seed, sampling_strategy={0.0: i, 1.0: train_num_pos})\n",
    "    X_train_res, y_train_res = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "    # Oversampling\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = sm.fit_resample(X_train_res, y_train_res)\n",
    "\n",
    "    # Initialize the model\n",
    "    rf = RandomForestClassifier(n_estimators = 100, \n",
    "                            random_state = random_state)\n",
    "    \n",
    "    # Calculate the f1 scores\n",
    "    rf.fit(X_train_res, y_train_res)\n",
    "    y_pred = np.rint(rf.predict(X_test))\n",
    "    f1_new = f1_score(y_test, y_pred, average='binary')\n",
    "    \n",
    "    # Append the values to look at later\n",
    "    under_over_sampling['param'].append(i) \n",
    "    under_over_sampling['f1_unov'].append(f1_new) \n",
    "\n",
    "    # Find the param with the best f1 score\n",
    "    if f1_new > f1:\n",
    "        f1 = f1_new\n",
    "        undersampling_parameter = i\n",
    "\n",
    "print(undersampling_parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ovun = pd.DataFrame.from_dict(under_over_sampling)\n",
    "df_data_ovun.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of the perfrmance of the model, after applying a combination of both under- and oversampling.\n",
    "sns.lineplot(data=df_data_ovun, x='param', y='f1_unov')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of all three experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all results from exclusive under- and oversampling, as well as the combination of both.\n",
    "df_all = pd.concat([df_data_ov, df_data_un, df_data_ovun], axis=1)\n",
    "df_all = df_all.drop('param', axis=1)\n",
    "df_all['param'] = df_data_ov['param']\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line plot of all performances\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.lineplot(data=df_all, x='param', y='f1_un', ax=ax)\n",
    "sns.lineplot(data=df_all, x='param', y='f1_ov', ax=ax)\n",
    "sns.lineplot(data=df_all, x='param', y='f1_unov', linewidth=3, ax=ax)\n",
    "\n",
    "ax.set_title('Model performance depending on under- and over sampling', size=14)\n",
    "ax.set_xlabel('Number of original values used', size=12, weight='bold')\n",
    "ax.set_ylabel('F1 score', size=12, weight='bold')\n",
    "\n",
    "ax.legend(['Undersampling', '_','Oversampling', '_', 'Combination'], loc=\"upper right\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "Now you should know what finetuning yield the best model performance.\n",
    "\n",
    "Have a nice day!\n",
    "\n",
    "/ Alicia"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rise-wildfires",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
